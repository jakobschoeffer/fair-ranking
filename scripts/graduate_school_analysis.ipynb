{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Synthetic Graduate School Admission Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9950,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import scipy\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import tikzplotlib\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9951,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/grad_school_synthetic_rescaled_theta_01_zeta_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain legitimate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9952,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_legit = data.copy()\n",
    "data_legit = data_legit.drop(['Gender','Score','Label'],1)\n",
    "Y = data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9953,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE_V</th>\n",
       "      <th>GRE_Q</th>\n",
       "      <th>GRE_AW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE_V  GRE_Q  GRE_AW\n",
       "0  148.0  144.0     3.0\n",
       "1  153.0  162.0     4.0\n",
       "2  151.0  156.0     3.5\n",
       "3  156.0  168.0     3.0\n",
       "4  154.0  159.0     4.0"
      ]
     },
     "execution_count": 9953,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_legit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RF classifier to obtain feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9954,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(random_state=1, criterion='gini', oob_score=True, min_samples_leaf=10)\n",
    "#model.fit(data_legit, Y)\n",
    "\n",
    "#print(model.oob_score_) #OOB score\n",
    "\n",
    "#result = permutation_importance(model, data_legit, Y, random_state=0, n_repeats = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9955,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale importances to sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9956,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(0.0609/(0.0609+0.3135+0.0278))\n",
    "#print(0.3135/(0.0609+0.3135+0.0278))\n",
    "#print(0.0278/(0.0609+0.3135+0.0278))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine SRCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9957,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spearman correlation\n",
    "#print(sp.stats.spearmanr(data['Gender'],data['GRE_V']))\n",
    "#print(sp.stats.spearmanr(data['Gender'],data['GRE_Q']))\n",
    "#print(sp.stats.spearmanr(data['Gender'],data['GRE_AW']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9958,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Score'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9959,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>GRE_V</th>\n",
       "      <th>GRE_Q</th>\n",
       "      <th>GRE_AW</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>153.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>156.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  GRE_V  GRE_Q  GRE_AW  Label\n",
       "0       0  148.0  144.0     3.0      0\n",
       "1       1  153.0  162.0     4.0      1\n",
       "2       0  151.0  156.0     3.5      0\n",
       "3       1  156.0  168.0     3.0      1\n",
       "4       1  154.0  159.0     4.0      1"
      ]
     },
     "execution_count": 9959,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test and separate label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9960,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.iloc[:800]\n",
    "test = data.iloc[800:]\n",
    "\n",
    "X_train = train.drop('Label',1)\n",
    "X_test = test.drop('Label',1)\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share of test labels with the positive outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9961,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_good_test = len(test[test.Label == 1])/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9962,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.535"
      ]
     },
     "execution_count": 9962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "share_good_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9963,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale training and test data to lie between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9964,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=col_names)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10007,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_scaled.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncate the test features to lie between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9966,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled[X_test_scaled > 1] = 1\n",
    "X_test_scaled[X_test_scaled < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data with legitimate features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9967,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_legit = X_train_scaled.drop('Gender',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9968,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_legit = X_test_scaled.drop('Gender',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9969,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_legit_array = X_train_scaled_legit.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute feature importances for legitimate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9970,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.71625\n",
      "Test score: 0.695\n",
      "OOB score: 0.6275\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=1, criterion='gini', oob_score=True, min_samples_leaf=10)\n",
    "model.fit(X_train_scaled_legit_array, y_train)\n",
    "    \n",
    "print('Training score:', model.score(X_train_scaled_legit,y_train)) #Training score\n",
    "print('Test score:', model.score(X_test_scaled_legit,y_test)) #Test score\n",
    "print('OOB score:', model.oob_score_) #OOB score\n",
    "    \n",
    "result = permutation_importance(model, X_train_scaled_legit, y_train, random_state=1, n_repeats = 10)\n",
    "#result = permutation_importance(model, X_test_scaled_legit, y_test, random_state=1, n_repeats = 10)\n",
    "#sorted_idx = result.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the feature importances to sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9972,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importances mean: [0.26803311 0.48955459 0.2424123 ]\n",
      "Importances std: [0.03659596 0.0456893  0.03174202]\n"
     ]
    }
   ],
   "source": [
    "factor = 1/np.sum(result.importances_mean)\n",
    "\n",
    "print('Importances mean:', np.multiply(result.importances_mean,factor))\n",
    "print('Importances std:', np.multiply(result.importances_std,factor))\n",
    "\n",
    "importance = np.multiply(result.importances_mean,factor) #Importance of legitimate features in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9973,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_legit = X_train_scaled_legit.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the absolute SRCC for legitimate features with gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9974,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: [0.03726864 0.23908502 0.13342886]\n"
     ]
    }
   ],
   "source": [
    "#Spearman correlation\n",
    "dep = np.zeros(len(col_names_legit))\n",
    "dep[0] = abs(sp.stats.spearmanr(X_train_scaled['Gender'],X_train_scaled['GRE_V'])[0])\n",
    "dep[1] = abs(sp.stats.spearmanr(X_train_scaled['Gender'],X_train_scaled['GRE_Q'])[0])\n",
    "dep[2] = abs(sp.stats.spearmanr(X_train_scaled['Gender'],X_train_scaled['GRE_AW'])[0])\n",
    "\n",
    "print('Spearman correlation:', dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9975,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_neg = [1-x for x in dep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train logistic regression models (with all features, and FTU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9976,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=0).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9977,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_legit_all = logreg.predict(X_test_scaled) #LogReg all test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9978,
   "metadata": {},
   "outputs": [],
   "source": [
    "legit_all_prob = logreg.predict_proba(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10008,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LogReg_all on test set: 0.965\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy LogReg_all on test set:', logreg.score(X_test_scaled, y_test)) #Test set accuracy for LogReg all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9980,
   "metadata": {},
   "outputs": [],
   "source": [
    "legit_all_prob_good = legit_all_prob.transpose()[1] #Probabilites of positive outcome for LogReg all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg FTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9981,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_ftu = LogisticRegression(random_state=0).fit(X_train_scaled_legit, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9982,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_legit_ftu = logreg_ftu.predict(X_test_scaled_legit) #LogReg FTU test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9983,
   "metadata": {},
   "outputs": [],
   "source": [
    "legit_ftu_prob = logreg_ftu.predict_proba(X_test_scaled_legit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9984,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LogReg_FTU on test set: 0.69\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy LogReg_FTU on test set:', logreg_ftu.score(X_test_scaled_legit, y_test)) #Test set accuracy for LogReg FTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9985,
   "metadata": {},
   "outputs": [],
   "source": [
    "legit_ftu_prob_good = legit_ftu_prob.transpose()[1] #Probabilities of positive outcome for LogReg FTU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the North Star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9986,
   "metadata": {},
   "outputs": [],
   "source": [
    "north_star = np.ones(len(X_test_scaled_legit.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9987,
   "metadata": {},
   "outputs": [],
   "source": [
    "#north_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute weighted distance ob test observations to the North Star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9988,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted Manhattan Distance\n",
    "def weightedManhattan(a,b,w):\n",
    "    q = a-b\n",
    "    return (w*np.absolute(q)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9989,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = list()\n",
    "X_test_scaled_legit_array = X_test_scaled_legit.to_numpy()\n",
    "\n",
    "for i in range(0,len(X_test_scaled_legit_array)):\n",
    "    dist = weightedManhattan(X_test_scaled_legit_array[i], north_star, importance*dep_neg)\n",
    "    temp.append(dist)\n",
    "    \n",
    "dist = temp\n",
    "dist_neg = [1-x for x in dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9990,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up new data frame and add (negative) distances to North Star plus ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9991,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new = X_test.copy()\n",
    "test_new['NEG_DIST_NORTH_STAR'] = dist_neg\n",
    "test_new['OUR_RANK'] = test_new['NEG_DIST_NORTH_STAR'].rank(ascending=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9992,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set $\\alpha$ and assign labels accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9993,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = list()\n",
    "cutoff = math.ceil(len(test_new)*share_good_test)\n",
    "dist_neg_ordered = test_new['NEG_DIST_NORTH_STAR'].sort_values(ascending=False)\n",
    "threshold_dist = dist_neg_ordered.iloc[cutoff]\n",
    "\n",
    "for i in test_new['NEG_DIST_NORTH_STAR']:\n",
    "    if (i > threshold_dist):\n",
    "        temp.append(1)\n",
    "    else:\n",
    "        temp.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add probabilities and ranks of LogReg models plus test labels to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9994,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new['LR_ALL_PROB_GOOD'] = legit_all_prob_good\n",
    "test_new['LR_ALL_RANK'] = test_new['LR_ALL_PROB_GOOD'].rank(ascending=False).astype(int)\n",
    "#test_new['LR_ALL_LABELS'] = pred_lr_all\n",
    "test_new['LR_FTU_PROB_GOOD'] = legit_ftu_prob_good\n",
    "test_new['LR_FTU_RANK'] = test_new['LR_FTU_PROB_GOOD'].rank(ascending=False).astype(int)\n",
    "#test_new['LR_FTU_LABELS'] = pred_lr_ftu\n",
    "test_new['ORIGINAL_LABELS'] = y_test\n",
    "\n",
    "test_new = test_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10011,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove sensitive information from new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9996,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_legit_new = X_test_scaled_legit.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9997,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_legit_new['LR_ALL_RANK'] = test_new['LR_ALL_RANK']\n",
    "X_test_scaled_legit_new['LR_FTU_RANK'] = test_new['LR_FTU_RANK']\n",
    "X_test_scaled_legit_new['OUR_RANK'] = test_new['OUR_RANK']\n",
    "X_test_scaled_legit_new['ORIGINAL_LABELS'] = test_new['ORIGINAL_LABELS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9998,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE_V</th>\n",
       "      <th>GRE_Q</th>\n",
       "      <th>GRE_AW</th>\n",
       "      <th>LR_ALL_RANK</th>\n",
       "      <th>LR_FTU_RANK</th>\n",
       "      <th>OUR_RANK</th>\n",
       "      <th>ORIGINAL_LABELS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.950</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>69</td>\n",
       "      <td>126</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>76</td>\n",
       "      <td>152</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.525</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>130</td>\n",
       "      <td>115</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>94</td>\n",
       "      <td>102</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE_V  GRE_Q    GRE_AW  LR_ALL_RANK  LR_FTU_RANK  OUR_RANK  ORIGINAL_LABELS\n",
       "0  0.950  1.000  0.818182            1            7         1                1\n",
       "1  0.625  0.500  0.636364           69          126        94                1\n",
       "2  0.600  0.425  0.636364           76          152       121                1\n",
       "3  0.525  0.650  0.727273          130          115        65                0\n",
       "4  0.100  0.600  0.363636           94          102       169                1"
      ]
     },
     "execution_count": 9998,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled_legit_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the share of unfairly treated observations ($\\mathcal{S}$) and the absolute number of occurences of meritocratic unfairness ($\\mathcal{T}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9999,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_LR_ALL = 0\n",
    "counter_LR_FTU = 0\n",
    "counter_OUR_METHOD = 0\n",
    "counter_ORIGINAL = 0\n",
    "\n",
    "#Uncomment the breaks for determining UNIQUE number of observations treated unfairly (S)\n",
    "#Comment out the breaks for determining number of instances where individual unfairness occurs (T)\n",
    "\n",
    "for i in range(0,len(X_test_scaled_legit)):\n",
    "    for j in range(0,len(X_test_scaled_legit)):\n",
    "        if(np.greater_equal(X_test_scaled_legit.iloc[i],X_test_scaled_legit.iloc[j]).all()):\n",
    "            if(np.greater(X_test_scaled_legit.iloc[i][0],X_test_scaled_legit.iloc[j][0])\n",
    "               or np.greater(X_test_scaled_legit.iloc[i][1],X_test_scaled_legit.iloc[j][1])\n",
    "               or np.greater(X_test_scaled_legit.iloc[i][2],X_test_scaled_legit.iloc[j][2])\n",
    "              ):\n",
    "                if(np.greater(X_test_scaled_legit_new.iloc[i][3],X_test_scaled_legit_new.iloc[j][3])):\n",
    "                    counter_LR_ALL = counter_LR_ALL+1\n",
    "                    break\n",
    "\n",
    "for i in range(0,len(X_test_scaled_legit)):\n",
    "    for j in range(0,len(X_test_scaled_legit)):\n",
    "        if(np.greater_equal(X_test_scaled_legit.iloc[i],X_test_scaled_legit.iloc[j]).all()):\n",
    "            if(np.greater(X_test_scaled_legit.iloc[i][0],X_test_scaled_legit.iloc[j][0])\n",
    "               or np.greater(X_test_scaled_legit.iloc[i][1],X_test_scaled_legit.iloc[j][1])\n",
    "               or np.greater(X_test_scaled_legit.iloc[i][2],X_test_scaled_legit.iloc[j][2])\n",
    "              ):\n",
    "                if(np.greater(X_test_scaled_legit_new.iloc[i][4],X_test_scaled_legit_new.iloc[j][4])):\n",
    "                    counter_LR_FTU = counter_LR_FTU+1\n",
    "                    break\n",
    "                    \n",
    "for i in range(0,len(X_test_scaled_legit)):\n",
    "    for j in range(0,len(X_test_scaled_legit)):\n",
    "        if(np.greater_equal(X_test_scaled_legit.iloc[i],X_test_scaled_legit.iloc[j]).all()):\n",
    "            if(np.greater(X_test_scaled_legit.iloc[i][0],X_test_scaled_legit.iloc[j][0])\n",
    "               or np.greater(X_test_scaled_legit.iloc[i][1],X_test_scaled_legit.iloc[j][1])\n",
    "               or np.greater(X_test_scaled_legit.iloc[i][2],X_test_scaled_legit.iloc[j][2])\n",
    "              ):\n",
    "                if(np.greater(X_test_scaled_legit_new.iloc[i][5],X_test_scaled_legit_new.iloc[j][5])):\n",
    "                    counter_OUR_METHOD = counter_OUR_METHOD+1\n",
    "                    break\n",
    "                    \n",
    "for i in range(0,len(X_test_scaled_legit)):\n",
    "    for j in range(0,len(X_test_scaled_legit)):\n",
    "        if(np.greater_equal(X_test_scaled_legit.iloc[i],X_test_scaled_legit.iloc[j]).all()):\n",
    "            if(np.greater(X_test_scaled_legit.iloc[i][0],X_test_scaled_legit.iloc[j][0])\n",
    "               or np.greater(X_test_scaled_legit.iloc[i][1],X_test_scaled_legit.iloc[j][1])\n",
    "               or np.greater(X_test_scaled_legit.iloc[i][2],X_test_scaled_legit.iloc[j][2])\n",
    "              ):\n",
    "                if(np.less(X_test_scaled_legit_new.iloc[i][6],X_test_scaled_legit_new.iloc[j][6])):\n",
    "                    counter_ORIGINAL = counter_ORIGINAL+1\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10000,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual (ranking) unfairness LogReg_all: 1631\n",
      "Individual (ranking) unfairness LogReg_FTU: 1364\n",
      "Individual (ranking) unfairness Our Method: 0\n",
      "Individual (ranking) unfairness Test Set: 1257\n"
     ]
    }
   ],
   "source": [
    "print('Individual (ranking) unfairness LogReg_all:', counter_LR_ALL)\n",
    "print('Individual (ranking) unfairness LogReg_FTU:', counter_LR_FTU)\n",
    "print('Individual (ranking) unfairness Our Method:', counter_OUR_METHOD)\n",
    "print('Individual (ranking) unfairness Test Set:', counter_ORIGINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label predictions of the LogReg models and our method for given threshold $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10001,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE_V</th>\n",
       "      <th>GRE_Q</th>\n",
       "      <th>GRE_AW</th>\n",
       "      <th>LR_ALL_RANK</th>\n",
       "      <th>LR_FTU_RANK</th>\n",
       "      <th>OUR_RANK</th>\n",
       "      <th>ORIGINAL_LABELS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.950</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>69</td>\n",
       "      <td>126</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>76</td>\n",
       "      <td>152</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.525</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>130</td>\n",
       "      <td>115</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>94</td>\n",
       "      <td>102</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE_V  GRE_Q    GRE_AW  LR_ALL_RANK  LR_FTU_RANK  OUR_RANK  ORIGINAL_LABELS\n",
       "0  0.950  1.000  0.818182            1            7         1                1\n",
       "1  0.625  0.500  0.636364           69          126        94                1\n",
       "2  0.600  0.425  0.636364           76          152       121                1\n",
       "3  0.525  0.650  0.727273          130          115        65                0\n",
       "4  0.100  0.600  0.363636           94          102       169                1"
      ]
     },
     "execution_count": 10001,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled_legit_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10002,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "alpha = [share_good_test]\n",
    "\n",
    "label_LR_ALL = list()\n",
    "label_LR_FTU = list()\n",
    "label_OUR_METHOD = list()\n",
    "\n",
    "for alpha in alpha:\n",
    "    cutoff = alpha*len(X_test)\n",
    "    for i in range(0,len(X_test)):\n",
    "        if(np.less_equal(X_test_scaled_legit_new.iloc[i][3],cutoff)):\n",
    "            label_LR_ALL.append(1)\n",
    "        else:\n",
    "            label_LR_ALL.append(0)\n",
    "        if(np.less_equal(X_test_scaled_legit_new.iloc[i][4],cutoff)):\n",
    "            label_LR_FTU.append(1)\n",
    "        else:\n",
    "            label_LR_FTU.append(0)\n",
    "        if(np.less_equal(X_test_scaled_legit_new.iloc[i][5],cutoff)):\n",
    "            label_OUR_METHOD.append(1)\n",
    "        else:\n",
    "            label_OUR_METHOD.append(0)\n",
    "    X_test_scaled_legit_new['LR_ALL_LABELS_' + str(alpha)] = label_LR_ALL\n",
    "    X_test_scaled_legit_new['LR_FTU_LABELS_' + str(alpha)] = label_LR_FTU\n",
    "    X_test_scaled_legit_new['OUR_LABELS_' + str(alpha)] = label_OUR_METHOD\n",
    "    \n",
    "    label_LR_ALL = list()\n",
    "    label_LR_FTU = list()\n",
    "    label_OUR_METHOD = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10003,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE_V</th>\n",
       "      <th>GRE_Q</th>\n",
       "      <th>GRE_AW</th>\n",
       "      <th>LR_ALL_RANK</th>\n",
       "      <th>LR_FTU_RANK</th>\n",
       "      <th>OUR_RANK</th>\n",
       "      <th>ORIGINAL_LABELS</th>\n",
       "      <th>LR_ALL_LABELS_0.535</th>\n",
       "      <th>LR_FTU_LABELS_0.535</th>\n",
       "      <th>OUR_LABELS_0.535</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.950</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>69</td>\n",
       "      <td>126</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>76</td>\n",
       "      <td>152</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.525</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>130</td>\n",
       "      <td>115</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>94</td>\n",
       "      <td>102</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE_V  GRE_Q    GRE_AW  LR_ALL_RANK  LR_FTU_RANK  OUR_RANK  \\\n",
       "0  0.950  1.000  0.818182            1            7         1   \n",
       "1  0.625  0.500  0.636364           69          126        94   \n",
       "2  0.600  0.425  0.636364           76          152       121   \n",
       "3  0.525  0.650  0.727273          130          115        65   \n",
       "4  0.100  0.600  0.363636           94          102       169   \n",
       "\n",
       "   ORIGINAL_LABELS  LR_ALL_LABELS_0.535  LR_FTU_LABELS_0.535  OUR_LABELS_0.535  \n",
       "0                1                    1                    1                 1  \n",
       "1                1                    1                    0                 1  \n",
       "2                1                    1                    0                 0  \n",
       "3                0                    0                    0                 1  \n",
       "4                1                    1                    1                 0  "
      ]
     },
     "execution_count": 10003,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled_legit_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the accuracy of our method on the test labels (for $\\alpha$ set to the share of observations with positive outcome in the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10004,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62\n"
     ]
    }
   ],
   "source": [
    "#Accuracy of our method\n",
    "#Look at alpha=0.75 (change above)\n",
    "\n",
    "our_labels = X_test_scaled_legit_new['OUR_LABELS_' + str(share_good_test)]\n",
    "original_labels = X_test_scaled_legit_new['ORIGINAL_LABELS']\n",
    "\n",
    "accuracy_ours = (our_labels == original_labels).mean()\n",
    "print(accuracy_ours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group fairness (demographic parity) for original LogReg predictions and our method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Admission ratio female:male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10005,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected females (LogReg All): 92\n",
      "Accepted females (LogReg All): 0\n",
      "Rejected males (LogReg All): 0\n",
      "Accepted males (LogReg All): 108\n",
      "Accepted Female/Male (LogReg All): 0.0\n",
      "----------\n",
      "Rejected females (LogReg FTU): 62\n",
      "Accepted females (LogReg FTU): 30\n",
      "Rejected males (LogReg FTU): 37\n",
      "Accepted males (LogReg FTU): 71\n",
      "Accepted Female/Male (LogReg FTU): 0.4225352112676056\n",
      "----------\n",
      "Rejected females (Our Method): 51\n",
      "Accepted females (Our Method): 41\n",
      "Rejected males (Our Method): 42\n",
      "Accepted males (Our Method): 66\n",
      "Accepted Female/Male (Our Method): 0.6212121212121212\n",
      "----------\n",
      "Rejected females (Test Labels): 89\n",
      "Accepted females (Test Labels): 3\n",
      "Rejected males (Test Labels): 4\n",
      "Accepted males (Test Labels): 104\n",
      "Accepted Female/Male (Test Labels): 0.028846153846153848\n"
     ]
    }
   ],
   "source": [
    "#Ratio of admitted males to females\n",
    "X_test_scaled_legit_new['Gender'] = X_test_scaled['Gender']\n",
    "X_test_scaled_legit_new['LR_ALL_LABELS'] = pred_legit_all\n",
    "X_test_scaled_legit_new['LR_FTU_LABELS'] = pred_legit_ftu\n",
    "\n",
    "\n",
    "print('Rejected females (LogReg All):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['LR_ALL_LABELS'] == 0)]))\n",
    "print('Accepted females (LogReg All):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['LR_ALL_LABELS'] == 1)]))\n",
    "print('Rejected males (LogReg All):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['LR_ALL_LABELS'] == 0)]))\n",
    "print('Accepted males (LogReg All):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['LR_ALL_LABELS'] == 1)]))\n",
    "print('Accepted Female/Male (LogReg All):', (len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['LR_ALL_LABELS'] == 1)]))/(len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['LR_ALL_LABELS'] == 1)])))\n",
    "print('----------')\n",
    "print('Rejected females (LogReg FTU):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['LR_FTU_LABELS'] == 0)]))\n",
    "print('Accepted females (LogReg FTU):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['LR_FTU_LABELS'] == 1)]))\n",
    "print('Rejected males (LogReg FTU):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['LR_FTU_LABELS'] == 0)]))\n",
    "print('Accepted males (LogReg FTU):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['LR_FTU_LABELS'] == 1)]))\n",
    "print('Accepted Female/Male (LogReg FTU):', (len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['LR_FTU_LABELS'] == 1)]))/(len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['LR_FTU_LABELS'] == 1)])))\n",
    "print('----------')\n",
    "print('Rejected females (Our Method):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['OUR_LABELS_' + str(share_good_test)] == 0)]))\n",
    "print('Accepted females (Our Method):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['OUR_LABELS_' + str(share_good_test)] == 1)]))\n",
    "print('Rejected males (Our Method):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['OUR_LABELS_' + str(share_good_test)] == 0)]))\n",
    "print('Accepted males (Our Method):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['OUR_LABELS_' + str(share_good_test)] == 1)]))\n",
    "print('Accepted Female/Male (Our Method):', (len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['OUR_LABELS_' + str(share_good_test)] == 1)]))/(len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['OUR_LABELS_' + str(share_good_test)] == 1)])))\n",
    "print('----------')\n",
    "print('Rejected females (Test Labels):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['ORIGINAL_LABELS'] == 0)]))\n",
    "print('Accepted females (Test Labels):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['ORIGINAL_LABELS'] == 1)]))\n",
    "print('Rejected males (Test Labels):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['ORIGINAL_LABELS'] == 0)]))\n",
    "print('Accepted males (Test Labels):', len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['ORIGINAL_LABELS'] == 1)]))\n",
    "print('Accepted Female/Male (Test Labels):', (len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['ORIGINAL_LABELS'] == 1)]))/(len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['ORIGINAL_LABELS'] == 1)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Admission rates by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10006,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admission rate females (LogReg All): 0.0\n",
      "Admission rate males (LogReg All): 1.0\n",
      "----------\n",
      "Admission rate females (LogReg FTU): 0.32608695652173914\n",
      "Admission rate males (LogReg FTU): 0.6574074074074074\n",
      "----------\n",
      "Admission rate females (Our Method): 0.44565217391304346\n",
      "Admission rate males (Our Method): 0.6111111111111112\n",
      "----------\n",
      "Admission rate females (Test Labels): 0.03260869565217391\n",
      "Admission rate males (Test Labels): 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "#Admission rates by gender\n",
    "\n",
    "reject_females_logreg_all = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['LR_ALL_LABELS'] == 0)])\n",
    "accept_females_logreg_all = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['LR_ALL_LABELS'] == 1)])\n",
    "reject_males_logreg_all = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['LR_ALL_LABELS'] == 0)])\n",
    "accept_males_logreg_all = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['LR_ALL_LABELS'] == 1)])\n",
    "\n",
    "reject_females_logreg_ftu = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['LR_FTU_LABELS'] == 0)])\n",
    "accept_females_logreg_ftu = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['LR_FTU_LABELS'] == 1)])\n",
    "reject_males_logreg_ftu = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['LR_FTU_LABELS'] == 0)])\n",
    "accept_males_logreg_ftu = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['LR_FTU_LABELS'] == 1)])\n",
    "\n",
    "reject_females_our = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['OUR_LABELS_' + str(share_good_test)] == 0)])\n",
    "accept_females_our = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['OUR_LABELS_' + str(share_good_test)] == 1)])\n",
    "reject_males_our = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['OUR_LABELS_' + str(share_good_test)] == 0)])\n",
    "accept_males_our = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['OUR_LABELS_' + str(share_good_test)] == 1)])\n",
    "\n",
    "reject_females_test = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['ORIGINAL_LABELS'] == 0)])\n",
    "accept_females_test = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 0) & (X_test_scaled_legit_new['ORIGINAL_LABELS'] == 1)])\n",
    "reject_males_test = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['ORIGINAL_LABELS'] == 0)])\n",
    "accept_males_test = len(X_test_scaled_legit_new[(X_test_scaled_legit_new['Gender'] == 1) & (X_test_scaled_legit_new['ORIGINAL_LABELS'] == 1)])\n",
    "\n",
    "print('Admission rate females (LogReg All):', accept_females_logreg_all/(accept_females_logreg_all+reject_females_logreg_all))\n",
    "print('Admission rate males (LogReg All):', accept_males_logreg_all/(accept_males_logreg_all+reject_males_logreg_all))\n",
    "print('----------')\n",
    "print('Admission rate females (LogReg FTU):', accept_females_logreg_ftu/(accept_females_logreg_ftu+reject_females_logreg_ftu))\n",
    "print('Admission rate males (LogReg FTU):', accept_males_logreg_ftu/(accept_males_logreg_ftu+reject_males_logreg_ftu))\n",
    "print('----------')\n",
    "print('Admission rate females (Our Method):', accept_females_our/(accept_females_our+reject_females_our))\n",
    "print('Admission rate males (Our Method):', accept_males_our/(accept_males_our+reject_males_our))\n",
    "print('----------')\n",
    "print('Admission rate females (Test Labels):', accept_females_test/(accept_females_test+reject_females_test))\n",
    "print('Admission rate males (Test Labels):', accept_males_test/(accept_males_test+reject_males_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
